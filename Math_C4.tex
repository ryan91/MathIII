\chapter{Lineare Abbildungen}

\gqm{Struktur erhaltende Abbildungen zwischen Vektorräumen}

\section{Definition: lineare Abbildung und Isomorphismus}
	Seien $V,W$ $k\rightarrow$ Vektorräume
	\begin{enumerate}[a)]
		\item $\alpha:V\rightarrow W$ heißt \textbf{lineare Abbildung} oder Vektorraum-Homomorphismus, falls:
			\subExBegin{(1)}
				\item $\alpha\lrr{v+w}=\alpha\lrr{v}+\alpha\lrr{w}\quad\forall v,w\in V$ (Additivität)
				\item $\alpha\lrr{k\cdot v}=k\cdot\alpha\lrr{v}\quad\forall v\in V,\forall k\in K$ (Homogenität)
			\end{enumerate}
		\item Ist die lineare Abbildung $\alpha: V\rightarrow W$ bijektiv, so heißt $\alpha$ \textbf{Isomorphismus}.\\
			$V$ und $W$ heißen \textbf{isomorph}, wenn $V\cong W$
	\end{enumerate}

\section{Bemerkung}
	Sei $\alpha:V\rightarrow W$ linear
	\begin{enumerate}[a)]
		\item $\alpha\lrr{\sigma}=\sigma$
		\item $\alpha\lrr{\limsum{i=1}{m}k_iv_i}=\limsum{i=1}{m}k_i\underbrace{\alpha\lrr{v_i}}_{\scriptstyle \in W}$
	\end{enumerate}
	\textbf{Beweis}
	\begin{enumerate}[a)]
		\item $\alpha\lrr{\sigma}=\alpha\lrr{\sigma+\sigma}=\alpha\lrr{\sigma}+\alpha\lrr{\sigma}\Rightarrow\underbrace{\alpha\lrr{\sigma}}_{\scriptstyle \in V}=\underbrace{\sigma}_{\scriptstyle \in W}$
		\item Induktion über m (Verwendet Additivität und Homogenität)
	\end{enumerate}

\section{Beispiel}
	\begin{enumerate}[a)]
		\item Nullabbildung
		\item $k\in K:\alpha:V\rightarrow V\quad v\mapsto k\cdot v$ (Speziell: $k=1,\alpha =\mbox{id}_v$)
		\item $\sigma:\mr^3\rightarrow\mr^3$\\
			$\lrv{x_1\\x_2\\x_3}\mapsto\lrv{x_1\\x_2\\-x_3}$\\
			Spiegelung an der $x_1,x_2$-Ebene.
		\item $\alpha:\mr^2\rightarrow\mr$\\
			$\lrv{x_1\\x_2}\mapsto x_1^2$ nicht linear\\
			Bsp: $\alpha\lrr{\lrv{1\\0}+\lrv{1\\0}}=\alpha\lrr{\lrv{2\\0}}=2^2=4$\\
			$\alpha\lrr{\lrv{1\\0}}=1^2=1$, das heißt $\alpha\lrr{\lrv{1\\0}}+\alpha\lrr{\lrv{1\\0}}=1+1=2\neq 4$\\
			Damit ist die Additivität verletzt.
		\item $\alpha:\mr^2\rightarrow\mr^2\quad \lrv{x\\y}\mapsto\lrv{y\\x}$ ist ein Isomorphismus
	\end{enumerate}

\section{Wichtige Sätze über Matrizen (Wdh.)}
	\begin{enumerate}[a)]
		\item $m\times n$-Matrizen über $K$ bilden $K$-Vektorräume (Nullvektor ist die Nullmatrix)
		\item $n\times n$-Matrizen über $K$ bilden einen Ring mit 1 $\lrr{1:E_n}$\\
			(für $n\geq 2$ nicht kommutativ)\\
			Bezeichnung: $M_n(K)$
		\item Elementare Zeilenumformungen ändern den Zeilenrang nicht.
		\item Transformationen einer Matrix durch elementare Zeilenumformungen auf Zeilenstufenform. Dann wird der Rang bestimmt.
		\item Gauß-Algorithmus zur Lösung von LGS $Ax=b$ bzw. um festzustellen, dass keine Lösung existiert
		\item $Ax=b$ lösbar $\Leftrightarrow\mbox{rg}\lrr{A}=\mbox{rg}\lrr{A|b}$\\
			Homogenes LGS $\lrr{b=0}:\dim\lrr{\mathbb{L}}=n-\mbox{rg}\lrr{A}$ Dabei ist die Anzahl der $n$ unbekannt.
	\end{enumerate}

\section{Definition: invertierbar}
	Sei $A$ eine $n\times n$-Matrix über $K$.\\
	$A$ heißt \textbf{invertierbar} falls $A$ in $M_n(K)$ bezüglich Multiplikation invertierbar ist. Das heißt es existiert $A^{-1}\in M_n(K)$ mit $A\cdot A^{-1}=A^{-1}\cdot A=E_n$ \\
	Zur Feststellung ob $A$ invertierbar ist und wie man $A^{-1}$ berechnet später mehr.

\section{Satz}
	Sei $A$ eine $m\times n$-Matrix über $K$.\\
	Definiere $\alpha:K^n\rightarrow K^m$ auch durch $\alpha\lrr{x}=A\cdot x$\\
	Dann ist $\alpha$ linear.\\
	Später werden wir sehen, dass alle linearen Abbildungen $\alpha:K^n\rightarrow K^m$ von dieser Form sind.

	\textbf{Beweis}

	Folgt aus den Rechenregeln für Matrizen.\\
	Zeige für $k\in K, v,w\in V$ beliebig: $A\cdot\lrr{k\cdot v}=k\cdot A\cdot v$\\
	$A\cdot\lrr{v+w}=A\cdot v+A\cdot w$

\section{Satz: Addition, Skalarmultiplikation und Hintereinanderausführung}
	Seien $U,V,W$ $K$-Vektorräume
	\begin{enumerate}[a)]
		\item $\alpha,\beta:U\rightarrow V$ linear, so sind auch $\lrr{\alpha+\beta}\lrr{u}:=\alpha\lrr{u}+\beta\lrr{v}$ und $\lrr{k\cdot\alpha}\lrr{u}=k\cdot\alpha\lrr{v}$ linear ($k\in K$ beliebig)
		\item $\alpha:V\rightarrow W,\gamma:W\rightarrow U$ linear, so ist auch $\gamma\circ\alpha:V\rightarrow U$ linear.\\
			(Schreibweise: $\gamma\alpha$ statt $\gamma\circ\alpha$)
	\end{enumerate}
	\textbf{Beweis} wird in Übungen behandelt

\section{Satz zu Untervektorräumen}
	Sei $\alpha:V\rightarrow W$ linear
	\begin{enumerate}[a)]
		\item Ist $U$ ein Untervektorraum von $V$, so ist $\alpha\lrr{U}$ ein Untervekorraum von $W$\\
			Insbesondere gilt $\alpha\lrr{V}$ (Bild von $V$) ist ein Untervektorraum von $W$
		\item Ist $U$ endlich dimensional, so ist $\dim\lrr{\alpha\lrr{U}}\leq\dim\lrr{U}$
	\end{enumerate}
	\textbf{Beweis}
	\begin{enumerate}[a)]
		\item Folgt aus 4.2
		\item Sei $u_1,\dots,u_k$ eine Basis von $U$.\\
			Dann ist $\alpha\lrr{u_1},\dots,\alpha\lrr{u_k}$ ein Erzeugendensystem von $\alpha\lrr{U}$ (Siehe 4.2)\\
			Das heißt $\lrc{\alpha\lrr{u_1},\dots,\alpha\lrr{u_n}}$ enthält eine Basis von $\alpha\lrr{U}$ Behauptung folgt.
	\end{enumerate}

\section{Definition: Rang}
	Sei $\alpha:V\rightarrow W$ linear, $V$ endlich dimensional\\
	Dann heißt $\dim\lrr{\alpha\lrr{U}}=:\mbox{rg}\lrr{\alpha}$ der \textbf{Rang} von $\alpha$\\
	Der Zusammenhang mit dem Rang-Begriff bei Matrizen kommt später.

\section{Satz: Kern}
	Sei $\alpha:V\rightarrow W$ lineare Abbildung
	\begin{enumerate}[a)]
		\item $\mbox{ker}\lrr{\alpha}:=\lrc{v\in V:\alpha\lrr{V}=\sigma}=\alpha^{-1}\lrr{\lrc{\sigma}}$, der \textbf{Kern} von $\alpha$, ist ein Untervektorraum von $V$
		\item $\alpha$ injektiv $\Leftrightarrow\mbox{ker}\lrr{\alpha}=\lrr{\sigma}$
		\item $\alpha$ bijektiv, so ist die Umkehrabbildung $\alpha^{-1}:W\rightarrow V$ ebenfalls (bijektiv) eine lineare Abbildung (Isomorphismus)
	\end{enumerate}
	\textbf{Beweis}
	\begin{enumerate}[a)]
		\item Nachrechnen
		\item
			\begin{enumerate}
				\item[$\Rightarrow$] Klar
				\item[$\Leftarrow$] Seien $v_1,v_2\in V$ beliebig mit $\alpha\lrr{v_1}=\alpha\lrr{v_2}$\\
					Dann ist $\underbrace{\alpha\lrr{v_1}-\alpha\lrr{v_2}}_{\scriptstyle =0}=\alpha\lrr{v_1-v_2}$\\
					Daraus folgt (da $\mbox{ker}=\lrc{\sigma}$) $v_1-v_2=\sigma$, also $v_1=v_2$
			\end{enumerate}
		\item Seien $w_1,w_2\in W$ beliebig.\\
			Dann $v_1,v_2\in V$ mit $\alpha\lrr{v_1}=w_1,\alpha\lrr{v_2}=w_2$\\
			Beziehungsweise $v_1=\alpha^{-1}\lrr{w_1}, v_2=\alpha^{-1}\lrr{w_2}$\\
			Dann:\\
			$\alpha^{-1}\lrr{w_1+w_2}=\alpha^{-1}\lrr{\alpha\lrr{v_1}+\alpha\lrr{v_2}}=\alpha^{-1}\lrr{\alpha\lrr{v_1+v_2}}=v_1+v_2=
			\alpha^{-1}\lrr{w_1}+\alpha^{-1}\lrr{w_2}$\\
			Homogenität funktioniert entsprechen
	\end{enumerate}

\section{Beispiel}
	$\alpha:\mr^3\rightarrow\mr^3$\\
	$\lrv{x_1\\x_2\\x_3}\rightarrow\lrv{x_1\\2x_1\\x_1+x_2+x_3}$ linear,\\
	da Multiplikation mit $A=\lrv{1&0&0\\2&0&0\\1&1&2}$\\
	$\alpha\lrr{e_2}=\lrv{0\\0\\1}, \alpha\lrr{e_3}=\lrv{0\\0\\2}$\\
	Das heißt $\left\langle e_2,e_3\right\rangle$ wird auf $\left\langle e_3\right\rangle\quad\dim\lrr{U}=2$\\
	$\dim\lrr{\alpha\lrr{U}}=1$\\
	$\mbox{ker}\lrr{\alpha}$: Löse LGS $\lrr{\begin{array}{ccc|c}1&0&0&0\\2&0&0&0\\1&1&1&0\end{array}}$\\
	Das liefert $\mbox{ker}\lrr{\alpha}=\lrc{\lrv{0\\-2c\\0}:c\in\mr}$

\section{Satz}
	Seien $V,W$ $k$-Vektorräume. $\dim\lrr{V}=n, \lrc{v_1,\dots,v_n}$ Basis von $V$ und $w_1,\dots,w_n\in W$ beliebig\\
	Dann existiert genau eine lineare Abbildung $\alpha:V\rightarrow W$ mit $\alpha\lrr{v_i}=w_i$ mit $i=1,\dots,n$\\
	Wenn man die Bilder der Basisvektoren kennt ist damit die lineare Abbildung genau bestimmt.\\
	Nämlich $\alpha\lrr{\limsum{i=1}{n}k_iv_i}:=\limsum{i=1}{n}k_i\underbrace{\alpha\lrr{v_i}}_{\scriptstyle w_i}$

	\textbf{Beweis}

	Das durch $\odot$ definierte $\alpha$ ist wohldefiniert, linear und erfüllt $\alpha\lrr{v_i}=w_i$\\
	$\alpha$ ist eindeutig: Ist $\beta\lrr{v_i}=w_i, i=1,\dots, n$ und $\beta: V\rightarrow W$ linear so gilt:\\
	$\beta\lrr{\limsum{i=1}{n}k_iv_i}\underset{\scriptstyle\beta\mbox{\scriptsize  lin.}}{=}\Sigma k_i\underbrace{\beta\lrr{v_i}}_{\scriptstyle w_i}\underset{\scriptstyle\alpha\mbox{\scriptsize  lin.}}{=}\alpha\lrr{\Sigma k_iv_i}\Rightarrow\beta=\alpha$

\section{Beispiel}
	Sei $V=\mr^2$ und $\alpha$ die Drehung um den Winkel $\varphi$, $0\leq\varphi\leq 2\pi$ um $\lrv{0\\0}$.\\
	Dabei ist $\alpha$ linear.

	%TODO - Graphik einfügen

	$e_1\mapsto\lrv{\cos\lrr{\varphi}\\\sin\lrr{\varphi}}$\\
	$e_2\mapsto\lrv{-\sin\lrr{\varphi}\\\cos\lrr{\varphi}}$

	Nach 4.10 gilt $\underbrace{x}_{\scriptstyle x_1e_1+x_2e_2}=\lrv{x_1\\x_2}\mapsto\underbrace{x_1\cdot\lrv{\cos\lrr{\varphi}\\\sin\lrr{\varphi}}+x_2\cdot\lrv{-\sin\lrr{\varphi}\\\cos\lrr{\varphi}}}_{\scriptstyle x_1\cdot\alpha\lrr{e_1}+x_2\cdot\alpha\lrr{e_2}}=\underbrace{\lrv{\cos\lrr{\varphi}&-\sin\lrr{\varphi}\\\sin\lrr{\varphi}&\cos\lrr{\varphi}}}_{\scriptstyle A}\cdot x$

  $K^n\rightarrow K^m$\\
  $A$ $m\times x$- Matrix über $K$.\\
  $\alpha:\begin{cases}K^n\mapsto K^m\\x\mapsto Ax\end{cases}$

  $
  \begin{pmatrix}
    a_{11}&\dots&a_{nn}\\
    \vdots&\ddots&\vdots\\
    a_{m1}&\dots&a_{mn}
  \end{pmatrix}\cdot
  \begin{pmatrix}
    x_1\\
    \vdots\\
    x_n
  \end{pmatrix}=
  \begin{pmatrix}
    a_{11}x_1+a_{12}x_2+...+a_{1n}x_n\\
    \vdots\\
    a_{m1}x_1+a_{m2}x_2+...+a_{mn}x_n
  \end{pmatrix}
  $

  $\alpha:V\rightarrow W, v_1,...,v_n$ Basis von $V$.\\
  Falls $\alpha(v_1)=w_i$ bekannt sind, $i=1,...,n$, dann ist $\alpha$
  bekannt:\\
  $v\in V$, $v=\limsum{i=1}{n}c_iv_i,\ c_i\in K$\\
  $\alpha(v)=\alpha\lrr{\limsum{i=1}{n}c_iv_i}=\limsum{i=1}{n}\alpha\lrr{c_iv_i}=\limsum{i=1}{n}c_i\alpha(v_i)=\limsum{i=1}{n}c_iw_i$

  $\alpha$ Drehung um $\varphi$ um $(0|0)$ (gegen Uhrzeigersinn)\\
  lineare Abbildung. $e_1,e_2$ kanonische Basis.

  $\alpha(e_1)=\cos(\varphi)e_1+\sin(\varphi)e_2$\\
  $\alpha(e_2)=-\sin(\varphi)e_1+\cos(\varphi)w_2$

  \section{Satz: Injektivität, Surjektivität und Bijektivität}

  $\alpha:V\rightarrow W$ linear, $\dim(V)=n$, $\lrc{v_1,...,v_n}$ Basis von $V$.

  \begin{enumerate}[a)]
    \item $\alpha$ injektiv $\Leftrightarrow$ $\alpha(v_1),...,\alpha(v_n)$
      sind linear unabhängig.
    \item $\alpha$ surjektiv $\Leftrightarrow$ $\alpha(v_1),...,\alpha(v_n)$
      Basis von $W$
    \item $\alpha$ bijektiv $\Leftrightarrow$ $\alpha(v_1),...,\alpha(v_n)$
      Basis von $W$.
  \end{enumerate}

  \textbf{Beweis:}
  \begin{enumerate}[a)]
    \item
      $\Rightarrow$: Angenommen, $c_1\alpha(v_1)+...+c_n\alpha(v_n)=\sigma$\\
      $=\alpha(c_1v_1,...,c_nv_n)$\\
      $c_1v_1+...+c_nv_n\in\mbox{ker}(\alpha)\overset{\mbox{\scriptsize 4.8b)}}{=}\lrc{\sigma}$, d.h. $c_1v_1+...+c_nv_n=\sigma$\\
      $\overset{v_1,v_n\mbox{\scriptsize lin. ua.}}{\Rightarrow} c_1=...=c_n=0$

      Also: $\alpha(v_1),\dots,\alpha(v_n)$ linear unabhängig.

      $\Leftrightarrow$: Zeige $\mbox{ker}(\alpha)=\lrc{\sigma}$\\
      (Dann fertig mit 4.8b)).\\
      $v\in\mbox{ker}(\alpha),\ v=\limsum{i=1}{n}d_iv_i,\ d_i\in K.\\
      \sigma=\alpha(v)=\alpha\lrr{\sum d_iv_i=\sum d_i\alpha(v_i}$

      $\Rightarrow$:\\
      $\alpha(v_1),...,\alpha(v_n)$ linear unabhängig $\Rightarrow v=\sum
      d_iv_i=\sigma$\\
      $\mbox{ker}(\alpha)=\lrc{\sigma}$.
     \item $v\in V, v=\limsum{i=1}{n}d_iv_i$\\
		$\alpha\lrr{v}=\limsum{i=1}{n}d_i\alpha\lrr{v_i}$\\
		$\alpha\lrr{V}=\left\langle\alpha\lrr{v_1},\dots,\alpha\lrr{v_n}\right\rangle_K$\\
		$\alpha$ surjektiv $\Leftrightarrow \alpha\lrr{V}=W \Leftrightarrow\left\langle\alpha\lrr{v_1},\dots,\alpha\lrr{v_n}\right\rangle_K=W$
	 \item Folgt aus a) und b)
	\end{enumerate}

\section{Korollar}
	Sei $V,W$ $K$-Vektorraum mit dim$(V)=$dim$(W)$\\
	Dann sind $V$ und $W$ isomorph.

	\textbf{Beweis}

	Sei $v_1,\dots,v_n$ Basis von $V$ und $w_1,\dots,w_n$ Basis von $W$

	Nach 4.10 gibt es genau eine lineare Abbildung $\alpha:V:\rightarrow W$ mit $\alpha\lrr{v_i}=w_i i=1,\dots,n$\\
	4.12c): $\alpha$ ist bijektiv\\
	$\alpha$ Vektorraum-Isomorphismus\\
	$V\cong W$

\section{Korollar}
  $V$ $n$- dimensionaler K-VR, $B=(v_1,...,v_n)$ geordnetet Basis von $V$. Dann
ist die Koordinatenabbildung $K_B\begin{cases}V\rightarrow
K^n\\v=\limsum{i=1}{n}c_iv_i\mapsto\lrv{c_1\\\vdots\\c_n}\end{cases}$ ein VR-
Isomorphismus. Also $V\cong K^n$.

  \textbf{Beweis}

  $K_B(v_i)=e_i=$\\
  $K_B$ lineare Abbildung, bildet $(v_1,...,v_n)$ auf die kanonische Basis
  $(e_1,...,e_n)$ von $K^n$ ab. Nach 4.12: $K_B$ Isomorphismus.

\section{Satz: Dimensionsformel}
	Sei $\alpha:V\rightarrow W$ eine lineare Abbildung\\
	Dann gilt $\dim\lrr{V}=\dim\lrr{\mbox{ker}\lrr{\alpha}}+\mbox{rg}\lrr{\alpha}$

	\textbf{Beweis}

	Sei $u_1,\dots,u_k$ Basis von $\mbox{ker}\lrr{\alpha}$\\
	Ergänze zu Basis $u_1,\dots,u_k,u_{k+1},\dots,u_n$ von $V$ (3.4b))\\
	$U:=\left\langle u_{k+1},\dots,u_n\right\rangle_K$\\
	$\mbox{ker}\lrr{\alpha}\mand U=\lrc{\sigma}:$\\
	$v\in\mbox{ker}\lrr{\alpha}\mand U\rightarrow v=\limsum{i=1}{k}c_iu_i=\limsum{i=k+1}{n}d_iu_i$\\
	$\limsum{i=1}{k}c_iu_i+\limsum{i=k+1}{n}\lrr{-d_i}u_i=\sigma\underset{\scriptstyle u_1,\dots,u_n}{\Rightarrow} c_1=\dots=c_k=d_{k+1}=\dots=d_n=0$\\
	linear unabhängig $\Rightarrow v=\sigma$

	$\alpha_{\mid U}$ ist injektiv, da $\mbox{ker}\lrr{\alpha_{\mid U}}\mand U=\lrc{0}$\\
	4.12a): $\alpha\lrr{u_{k+1}},\dots,\alpha\lrr{u_n}$ sind linear unabhängig.\\
	Das heißt $\dim\lrr{U}=\dim\lrr{\alpha\lrr{\alpha}\lrr{U}}$\\
	Klar: $\alpha\lrr{V}=\alpha\lrr{U}$\\
	$v=\limsum{i=1}{n}c_iu_i$\\
	$\alpha\lrr{v}=\limsum{i=1}{k}c_i\underbrace{\alpha\lrr{v_I}}_{\scriptstyle\sigma}+\limsum{i=k+1}{n}c_i\alpha\lrr{v_i}=\limsum{i=k+1}{n}c_i\alpha\lrr{u_i}\in\alpha\lrr{U}$\\

	$\dim(V)=\dim(\mbox{ker}(\alpha))+\dim\lrr(U)=\dim\lrr(\mbox{ker}\lrr(\alpha))+\dim(\alpha(U))=\dim(\mbox{ker}(\alpha))+ \dim(\alpha(V))=\dim(\mbox{ker}(\alpha)+\mbox{rg}(\alpha)$

\section{Korollar}
	Seien $V,W$ endlich dimensionale $K$-Vektorräume mit $\dim\lrr{V}=\dim\lrr{W}$ und $\alpha:V\rightarrow W$ eine lineare Abbildung.\\
	Dann gilt $\alpha$ injektiv $\Leftrightarrow\alpha$ surjektiv $\Leftrightarrow\alpha$ bijektiv

	\textbf{Beweis}

	$\dim\lrr{V}=\dim\lrr{W}=n$\\
	4.15: $n=\dim\lrr{\mbox{ker}\lrr{\alpha}}+\mbox{rg}\lrr{\alpha}$\\
	$\alpha$ injektiv $\Leftrightarrow\dim\lrr{\mbox{ker}\lrr{\alpha}}=0\Leftrightarrow\mbox{rg}\lrr{\alpha}=n\Leftrightarrow\alpha\lrr{V}=W$\\
	$\dim\lrr{W}=n\Leftrightarrow\alpha$ surjektiv

\section{Zusammenhang linearer Abbildungen und homogenen LGS}
	Sei $A$ eine $m\times n$-Matrix über $K$.\\
	Gesucht sind alle $x\in K^n$ mit $A\cdot x=0$ (Homogenes LGS, $m$ Gleichungen und $n$ Unbekannte)\\
	Betrachte $\alpha:\begin{cases}K^n\rightarrow K^m\\x\mapsto A\cdot x\end{cases}$ als lineare Abbildung\\
	Lösungsraum von $A\cdot x=0 \Rightarrow \mbox{ker}\lrr{\alpha}$\\
	4.15: $\dim\lrr{\mbox{ker}\lrr{\alpha}}=\dim\lrr{K^n}-\mbox{rg}\lrr{\alpha}=n-\mbox{rg}\lrr{\alpha}$\\
	Was ist $\mbox{rg}\lrr{\alpha}$?\\
	$\alpha\lrr{K^n}=\left\langle\alpha\lrr{e_1},\dots,\alpha\lrr{e_n}\right\rangle=\left\langle A\cdot e_1,\dots, A\cdot e_n\right\rangle_K$ )Da $e_1,\dots,e_n$ kanonische Basis von $K^n$)\\
	$A\cdot e_i=i$-te Spalte von $A$\\
	$\mbox{rg}\lrr{\alpha}=\dim\lrr{\alpha\lrr{K^n}}=\dim\left\langle \underbrace{S_1,\dots,S_n}_{\mbox{\scriptsize Spalten von }\scriptstyle A}\right\rangle_K=$ Maximale Anzahl linear unabhängiger Spalten von $A =$ Spaltenrang von $A$\\
	Die Dimension des Lösungsraums von $A\cdot x=0$. $n$-Spaltenrang von $A$\\
	Mathe II: Die Dimension des Lösungsraums von $A\cdot x=0$. $n$-Zeilenrang von $K$

\section{Korollar: Spalten- \& Zeilenrang}
	Sei $A$ eine $m\times n$-Matrix über $K$\\
	Dann ist Spaltenrang von $A =$ Zeilenrang von $A=\mbox{rg}\lrr{A}$\\
	Dabei ist $\mbox{rg}\lrr{A}=\mbox{rg}\lrr{\alpha}$, wobei $\alpha:K^n\rightarrow K^m$ definert ist durch $\alpha\lrr{x}=A\cdot x$
